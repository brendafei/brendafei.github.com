---
title: Hashing
categories: 数据结构
---

* toc
{:toc}


# 概述

---

哈希 / 散列。任意长度的输入 -散列函数-> 固定长度的输出。  

**为了解决什么问题**  
在查找中避免来回搜查，通过对参数K做某种算数运算，计算一个函数f(K)，直接得出表中K和相关数据的地址。  

**避免重复值的函数是十分稀有的**  
假设有n个元素要映射到m个地址中，那么存在mn种可能，其中m!种可以对每个元素给出不同的值，因此不产生重复值的占比为 m! / mn。eg. n = 31, m = 41，那么这个占比大概是一千万分之一（生日悖论 birthday paradox，23个人中出现同月日出生的人的概率大概为0.4927，<n=23, m=365>）。且，如果出现元素增减，原先能够避免出现重复值的函数大概率将不再适用，需要重新寻找新的函数。

**允许重复值出现并解决其带来的冲突问题**  
引入hashing / scatter storage，散列 / 分散存储  
so，使用hash table时需考虑两部分：1、散列函数；2、冲突collision解决方法  

> The verb "to hash" means to chop something up or to make a mess out of it; the idea in hashing is to scramble some aspects of the key and to use this partial information as the basis for searching.

平均查找时间：O(1)  
最坏查找时间：O(n)，所有的K都映射到同一个桶，通常好的hash function可以避免这种情况

# 散列函数

---

what is a good hash function? A good hash function satisfies (approximately) the assumption of simple uniform hashing。
尽可能的随机，尽可能的均匀，避免冲突或者使冲突次数最小化。以及 fast to compute    
-> **简单均匀散列**：每个关键字都被等可能地散列到m个槽位中的任何一个，并与其他关键字已散列到那个槽位无关。但是实际一般无法验证。

## *直接寻址表 direct-address table

使用一个数组-直接寻址表，其中的每个位置-槽slot，指向集合中一个关键字为k的元素，如果实际集合中没有关键字为k的元素，则该槽为空槽。  
当关键字的全域U比较小、且没有两个元素具有相同的关键字，可以采用此法，简单有效。如果全域U很大，实际存储的关键字集合K相对U来说可能很小，将浪费大量空间。  

直接寻址表的search, insert, delete操作都为O(1)。

## 除法散列法

**h(k) = k mod m**，m就是槽数
- m的选取：不太接近2的整数幂的素数，通常比较好。eg. 2000个元素，如果可以接受一次不成功的查找需要平均检查3个元素，2000 / 3 = 666.667，那么m可以选701，接近666.667、且不接近2的任何次幂的素数

## 乘法散列法

**h(k) = ⌊m (kA mod 1)⌋**
- 关键字k先乘上一个常数A (0 < A < 1)，提取kA的小数部分，以 kA mod 1 表示
- m乘以上面的小数部分的值，再向下取整。
- m的选取：不是特别关键，一般选择2的某个次幂，2p
- A的选取：Knuth建议A取值为 \(\sqrt{5}-1\)/2 = 0.6180339887…

<img src="/assets/images/hashing-multiply.png" width="400" />


**计算步骤**

- 假设计算机的字长为w位，关键字k正好可以用一个字表示，即，k < 2w
- 选择一个整数s，0 < s < 2w，使得 A = s / 2w
- 那么 kA mod 1 = (k * s / 2w) mod 1
  - k * s是一个2w位的值，可以用 r1 * 2w + r0 表示，r1为高w位，r0为低w位
  - / 2w 就是右移>>w位，此时r0为小数部分
  - mod 1取小数部分，即为r0
- m (kA mod 1)：
  - m = 2p，因此乘以m即为将r0左移<<p位
- 所以h(k)即为r0的高p位


## 全域散列法 universal hashing

不管关键字都有什么，随机的选择散列函数 -> 散列函数独立于要存储的关键字  
- 事先精心设计一组散列函数，实际执行时，从这组函数中随机选择一个

**全域universal的含义**  
使用函数组中随机选择的一个散列函数，两个不相等的关键字发生冲突的概率不大于 1 / m（m还是映射的槽数）
> 意会：也就是说，假设是链接法处理冲突，那么每个链表的长度最大不超过关键字全集长度n/m（实际证明时需区分要散列的关键字k是否已经存在于其对应的链表中，期望的链表长度分别为不存在时n/m和存在时n/m+1，数学证明…不考虑了，意会意会）

**解决什么问题**  
任何一个特定的散列函数都有可能出现关键字全部散列到同一个槽中的最坏情况，要避免这种可能，尽量使得平均性能良好

**设计一个全域散列函数类**
- 选择一个素数p，使得每个关键字k都在 [1, p - 1] 范围内，so，p > m（假定关键字全域大于m）
- 选择两个数a，b，使得a ∈ 集合Zp* {1, 2, …, p-1}，b ∈ 集合Zp {0, 1, 2, …, p-1}
- 定义散列函数
  - **hab(k) = ((ak + b) mod p) mod m**
  - （一次线性变化再加上模p和模m的归约）
- 散列函数簇为
  - **Hpm = {hab: a ∈ Zp*, b ∈ Zp}**，总共包含p(p-1)个散列函数

**证明全域universal**
- 两个不相等的关键字 k ≠ l，计算 r = (ak + b) mod p, s = (al + b) mod p，一定有 r ≠ s
- 因为 r - s ≡ a (k - l) mod p，a和(k - l)模p均不为0，而p为素数，所以乘积模p结果也不为0，so，r ≠ s
- 于是，对于某个给定的r，s的可能取值就位余下的 p - 1 种（模p的结果在[0, p-1]中）
- 而能够满足 s ≠ r 且 s ≡ r mod m 的s数量最多为：
  - p / m - 1 <= ((p + m - 1) / m) - 1 = (p - 1) / m
- so，s和r发生冲突的概率最多为 (p - 1) / m / (p - 1) = 1 / m


## 完全散列 perfect hashing

Although hashing is often a good choice for its excellent
<u>average-case</u> performance, hashing can also provide excellent
<u>worst-case</u> performance when the set of keys is static

**静态 static 含义**  
一旦各个关键字存入表中，关键字集合就不再变化。比如编程语言中的保留字集合等

**完全 perfect 含义**  
查找时，最坏情况下需要 O(1) 次内存访问

采用两级散列来设计完全散列方案，每一级都使用全域散列（universal hashing）
- 一级散列表
  - 从某一全域散列函数簇中精心选出一个散列函数 h，将 n 个关键字散列到 m
  个槽中，设置 m = n
- 二级散列表 secondary hash table Sj 的大小 mj
  - 散列到一级散列表槽 j 中的关键字数 nj 的平方，确保二级散列表中不出现冲突
- so，因为不需要处理冲突，查找时间为常数。定位一级散列表槽 + 定位二级散列表槽。

<img src="/assets/images/hashing-perfect-hashing.png" width="500" />

**怎么确保二级散列表不出现冲突**  
从全域散列函数类中随机选散列函数 h ，将 n 个关键字存储在一个大小为 m^2^ 的散列表中，表中出现冲突的概率小于 1/2  
$E\lbrack x\rbrack = \binom{n}{2} \cdot \frac{1}{m} = \binom{n}{2} \cdot \frac{1}{n^{2}} = \frac{n^{2} - n}{2} \cdot \frac{1}{n^{2}} < \frac{1}{2}$
（1/m为全域散列每一对关键字出现冲突的概率）  
so，1/2这么大的概率，m = n^2^ 时，只要随机多选几次，就能选到一个不产生冲突的 h

**怎么确保总体存储空间的期望数为 O(n)** （given that 二级散列表的大小依赖 n~j~ 平方）  
- 设置一级散列表大小 m = n，则有二级散列表大小总和的<u>期望</u>为：
  - $$\begin{matrix}
  E\lbrack\overset{m - 1}{\sum_{j = 0}}n_{j}^{2}\rbrack = & E\lbrack\overset{m - 1}{\sum_{j = 0}}n_{j} + 2\binom{n_{j}}{2}\rbrack = E\lbrack n\rbrack + 2\underset{¯}{E\lbrack\overset{m - 1}{\sum_{j = 0}}\binom{n_{j}}{2}\rbrack} \\
  = & n + 2\underset{¯}{\binom{n}{2}\frac{1}{m}} = n + 2\frac{n(n - 1)}{2m} = n + 2\frac{n(n - 1)}{2n} \\
  < & 2n \\
  \end{matrix}$$

  - $$E\lbrack\overset{m - 1}{\sum_{j = 0}}\binom{n_{j}}{2}\rbrack$$
  就是<u>一级</u>散列表中发生冲突的关键字的总对数，也就是
    $$\binom{n}{2}\frac{1}{m}$$
    - 从每一个 n~j~ 中选2的组合的总和 = 从所有的 n 中选2的组合的总和中，产生冲突的那部分（因为产生冲突，才能被归到同一个
    n~j~ 中，不在一个 n~j~ 中的元素不组合）

- 推论：一级散列表大小 m = n， 二级散列表大小 m~j~ = n~j~^2^ 时，所有二级散列表存储总量 \>= 4n 的概率小于 1/2。证明省略…
  - so，again，只要试几次，就能在全域散列函数类中找到一个存储量较为合理的函数


## 动态散列 dynamic hashing

- 散列表本身可伸缩，伸缩的同时伴随着散列函数的变化以及某些元素的重新分布
- 解决什么问题
  - 当数据量随时间增长，某些bucket上的冲突越来越多时，可能造成性能越来越糟，直接根据预期的数据量一次性扩展散列表并更换散列函数，可能造成空间浪费；阶段性的更换散列函数、扩展散列表、重新分布元素，每次的成本都很客观。
  - 使用动态散列，空间增长可控，且每次扩展时的性能损耗也可接受
- Extendible hashing 可扩展散列

  <a href="https://www2.cs.sfu.ca/CourseCentral/354/zaiane/material/notes/Chapter11/node20.html">
    <img src="/assets/images/hashing-extendible-hashing.png" width="200" />
  </a>
  
  - 新增一层 directory（hash table），包含指向bucket 的索引
  - local depth & global depth，散列表大小是2global depth。global depth同时也是寻址所需bit数
  - 每次overflow时，即要插入的 bucket bi 已满时，分两种情况处理：
    - 1、bi 的local depth = global depth
      - 散列表扩展一倍，通过copy自身的方式，也就是新增的directory索引还指向已有的那些bucket
      - global depth + 1
      - 拆分发生overflow的bucket，自己以及拆分出来的bucket的local depth + 1
      - 将拆分出来的bucket挂在对应的directory索引下
      - 更新散列函数
      - 重新分布overflow的bucket中的元素
    - 2、bi 的local depth < global depth
      - 直接拆分发生overflow的bucket，自己以及拆分出来的bucket的local depth + 1
      - 将拆分出来的bucket挂在对应的directory下
      - 重新分布overflow的bucket中的元素


## Linear hashing
- Witold Litwin, 1980
- 运行过程按照round进行，每个round同时使用两个散列函数，h 和 h’
- next指针指向发生overflow时需要拆分的bucket，这个bucket不一定是发生overflow的那个bucket
- 每次拆分结束后，next指针向下移动一个bucket
- round开始时，next指针指向散列表中的第一个bucket，所有bucket使用本轮的初始散列函数 h
- 计算一个元素所在bucket：
  - 使用本轮的初始散列函数 h，得出需要将元素放在某个bucket bi 中
  - 如果这个 bi 本轮中已经被拆分过，那么需要再使用本轮新的散列函数 h’ 重新计算一次元素应该插入的bucket（其实就是回归一下拆分时的redistribute过程）
- 发生overflow时：
  - 拆分next指针指向的bucket，自己和拆分出来的bucket使用新的散列函数 h’
  - 拆分出来的bucket放在所有bucket后面
  - 根据新的散列函数 h’，重新分布被拆分的bucket中的元素
  - next指针指向下一个bucket
  - 如果next指针已经超过本轮开始时的bucket范围，则开始下一轮次
    - 下一轮次中，初始散列函数为本轮的新散列函数 h’
- 重新开始一个round的时机：所有这个round开始时存在的bucket都已经被split过了  

  <a href="https://www.youtube.com/watch?v=HcVyagxx_b4">
    <img src="/assets/images/hashing-linear-hashing.png" width="400" />
  </a>

- 与extendible hashing相比
  - extendible hashing指数增长，linear hashing每次增长一个bucket
  - extendible hashing需要维护directory结构、global depth、local depth等额外信息
  - linear hashing需要维护next指针、每个bucket使用的散列函数
- 收缩
  - 如果在 controlled split 下，装载因子下降到阈值以下时，将触发合并操作
  - 合并操作撤销最后一次拆分，并重置文件状态

## Hashing variable-length data

> So far we have considered how to hash one-word keys. Multiword or variable-length keys can be handled by multiple-precision extensions of the methods above, but it is generally adequate to speed things up by combining the individual words together into a single word, then doing a single multiplication or division as above.



# 冲突解决

---

## 链接法 chaining

散列到同一个槽中的所有元素放在一个链表中，散列表中存放指向链表表头的指针。链表最好设计为双向链表，可以使得删除操作时间为O(1)（前提是删除的参数为元素本身而不是其关键字k）。

<mark>装载因子load factor α</mark>：n个元素，m个槽位，α = n / m，即一个链的平均元素数。

查找时间：O(1 + α)，成功查找和失败查找都是这个时间（证明方式不同…）
- **如果散列表中的槽数m和表中的元素n成正比，则有 n = O(m)，从而  α = n / m = O(m) / m = O(1)**

采用双向链表，insert最坏情况也为O(1)，delete最坏情况也为O(1)


## 开放寻址法 open addressing

所有元素直接存放在散列表中，散列表有可能被填满导致不能继续插入新元素。装载因子α绝不会超过1。（？因为设计时一定要考虑表中可以存放下所有的元素，so 槽位m >= 关键字全集个数n）

优点：无需存储指针，直接存储元素，节约空间（从而可以将更多的空间用于槽位，减少冲突）

**探查probe**  
连续检查散列表，找到空槽来放置待插入的关键字
- 散列函数增加一个参数-探查号（从0开始），对于每个关键字k，开放寻址的探查序列 probe sequence为：
  - < h(k, 0), h(k, 1), … h(m - 1) >
  - 探查序列为 {0, 1, …, m - 1} 的一个排列

**均匀散列 uniform hashing**  
每个关键字的探查序列等可能地为 {0, 1, …, m - 1} 的 m! 种排列中的任一种。真正的均匀散列难以实现，只能近似。


**计算探查序列的方法**
- 线性探查 linear probing
  - 探查方法
    - 新增一个辅助散列函数 auxiliary hash function，h’: U → {0, 1, …, m - 1}
    - 散列函数为：
      - h(k, i) = (h’(k) + i) mod m, i = 0, 1, …, m - 1，i为探查序号
    - 由辅助散列函数给出初始探查位置 T\[h<span dir="rtl">’</span>(k)\]，如果被占用，就连续探查。探查序列为 T[h’(k)], T[h’(k) + 1]… T[m - 1], T[0], T[1], …, T[h’(k) - 1]

  - 探查序列数
    - m种（只由初始的探查位置决定）
  - 缺点
    - 一次群集 primary clustering。随着连续被占用的槽不断增加，平均查找时间也随之不断增加

- 二次探查 quadratic probing
  - 探查方法
    - 散列函数为：
      - h(k, i) = (h’(k) + c~1~i + c~2~i^2^) mod m, i = 0, 1, …, m - 1, c~1~和c~2~为正的辅助常数
    - 初始探查位置为 T\[h<span dir="rtl">’</span>(k)\]
    - 探查偏移量依赖 i 平方，效果比线性探查好很多，但是为了充分利用散列表，c~1~、c~2~和m的值要受到限制
  - 探查序列数
    - m种（只由初始的探查位置决定）
  - 缺点
    - 二次群集 secondary clustering。如果两个关键字的初始探查位置相同，那它们的探查序列也相同。（线性探查也是如此）

- 双重散列 double hashing
  - 探查方法
    - 使用两个辅助散列函数 h~1~ 和 h~2~，双重散列的散列函数为：
      - h(k, i) = (h~1~(k) + ih~2~(k)) mod m, i = 0, 1, …, m - 1
    - 初始探查位置为 T\[h~1~(k)\]，然后每次跳 h~2~(k) 步继续找
      - so 为了能查找到整个散列表，h~2~(k)
        必须要与m（也即表的大小）互素。常用方法：取m为2的幂，设计
        h~2~ 总产生奇数；或者取m为素数，设计 h~2~
        总是返回比m小的正整数
  - 探查序列数
    - 𝚯(m^2^)。因为每一对 (h~1~(k), h~2~(k))
      都会产生一个不同的探查序列
  - 优点
    - 开放寻址法的最好方法之一，所产生的的排列具有随机选择排列的许多特性，非常接近“理想的”均匀散列的性能

**开放寻址法时间分析**  
分析在均匀散列的前提下，开放寻址法进行散列的期望探查次数（区分插入和查找）

- 查找 - 一次失败的探查
  - 第一次探查槽被占用的概率为 n /
    m。m个槽，n个元素，在本次探查的槽中有元素的概率
  - 第二次探查槽被占用的概率为 n - 1 / m -
    1。理解：在第二次探查时，本次可选的探查槽在剩下的 m - 1
    个中，由于第一次已经排除了一个被占用的槽、也就是一个元素，那么本次可能出现在剩下槽中的元素就减少为
    n - 1 个，也就是说问题变成，在剩下的 m - 1 个槽中，n - 1
    个元素出现在本次探查的槽中的概率是多少
  - 以此类推，第 j 次探查到被占用的槽的概率为 (n - j + 1) / (m - j + 1) \<= n / m
    - 因为对 n \< m，0 \<= x \< m，必有 (n - x) / (m - x) \<= n / m
  - 因此对于一次不成功的探查，除了最后一次，前面的每次一探查到的都是被占用的槽，这个概率为：
    - $$\frac{n}{m} \cdot \frac{n - 1}{m - 1} \cdot \frac{n - 2}{m - 2}\ldots\frac{n - i + 2}{m - i + 2} \leq (\frac{n}{m}){}^{i - 1} = \alpha^{i - 1}$$
  - 探查期望数的界为： 
    - $$E\lbrack x\rbrack \leq \overset{\infty}{\sum_{i = 1}}\alpha^{i - 1} = \frac{1}{1 - \alpha}$$
  - so，对于一次不成功的查找，其期望的探查次数至多为 1 / (1 - α)

- 插入
  - 如果采用的是均匀散列，<u>平均情况下</u>，向一个装载因子为α的开放寻址散列表中<u>成功插入</u>一个元素，至多需要做 1 / (1 - α) 次探查（和失败查找类似，成功的实际上就是除了最后一次前面的都失败）

- 查找 - 一次成功的探查
  - 如果 k 是第 i + 1 个被插入表中的关键字，则对 k
    的一次查找中，探查的期望次数至多为
    - 1 / (1 - i / m) = m / (m - i)
    - 和失败的查找不同，成功的查找的前提是 k 在表中，因而就有k是第几个被插入的这个额外信息，从而可以比失败的情况更加精确的分析
      - 探查序列和插入序列相同
      - 越靠前插入，相对来说插入的探查次数就越小，所以在查找的时候所需要的探查次数也越小
    - so，探查期望次数为（对散列表中的n个元素求平均）： 
      - $$\frac{1}{n}\overset{n - 1}{\sum_{i = 0}}\frac{m}{m - i} = \ldots \leq \ldots = \frac{1}{\alpha}\ln\frac{1}{1 - \alpha}$$

# 其他散列技术

---

## 一致性哈希 Consistent Hashing

- [<u>David Karger et al. at MIT,1997</u>](https://dl.acm.org/doi/pdf/10.1145/258533.258660)。用于分布式缓存、服务器数量发生变化时如何分发请求。
  - 为了解决什么问题：
    - 减少或消除分布式网络中的热点 hotsopt 问题
    - 避免在服务器数量出现增减时，需要重新映射所有key
- 散列表大小发生变化时，平均只有 n/m 个关键字需要重新映射。n为元素数，m为槽数
- 与 linear hashing 相比：linear hashing 顺序增减槽位，而 consistent hashing 可以在任意位置增减
- 应用：
  - 分布式缓存 distributed caching
  - 服务器数量发生变化时如何分发请求
  - 解决在对等网络（例如分布式哈希表）中跟踪文件的技术挑战
  - 分布式数据库
  - 服务器集群的负载均衡
  - 分布式散列表 distributed hash table, DHT
- 实现：
  - 抽象出一个hash ring。散列key到ring上，散列bucket到ring上，两个散列函数不一定相同
  - 定位某个key应该位于哪个bucket上：顺时针方向找到离这个key在ring上位置最近的bucket
  - 具体实现时，通常使用二分查找（这种情况bucket使用binary search tree维护）或者线性查找的方法，复杂度分别为 O(logN) 和 O(N)
  - 找到第一个比key的散列值大的bucket，找不到则在第一个bucket
- bucket增加
  - 虚拟节点的引入，可以使新的bucket平均分担之前每个bucket上的key
  - 且只有被分配到新的bucket上的那些key需要重新映射
- bucket减少
  - 只影响之前散列到这个bucket上的那些key
- 虚拟节点：
  - 避免数据倾斜 skewness ，key的不均匀分配（虽然所选的散列函数理论上可能是均匀的，但是实际场景不一定如此，而且bucket增删也可能造成倾斜问题）
  - 为每个bucket新增虚拟节点，可以使用新的函数将这些虚拟节点散列到ring上
  - 虚拟节点和真实的bucket一样工作
  - weight：bucket权重。不同的bucket权重对应不同数量的虚拟节点

  <img src="/assets/images/hashing-consistent-hashing.png" width="300" />

- [<u>复杂度分析</u>](https://en.wikipedia.org/wiki/Consistent_hashing#Complexity)
  - n元素数，m槽位数。
  - O( n / m ) 表示redistribution的平均复杂度；
  - O( log m ) 表示找到使用二叉搜索树时找到后继节点的复杂度
  
|               | Classic hash table | Consistent hashing |
|---------------|--------------------|--------------------|
| add a node    | O(n)               | O( n / m + log m ) |
| remove a node | O(n)               | O( n / m + log m ) |
| add a key     | O(1)               | O( log m )         |
| remove a key  | O(1)               | O( log m )         |

## [<u>SimHash</u>](https://www.zhihu.com/question/26762707/answer/890181997)

- SimHash是google用于海量文本去重的一种方法，它是一种局部敏感hash：如果两个字符串具有一定的相似性，那么在hash之后，仍然能保持这种相似性。

## GeoHash

- 将地球最为一个二维平面分解，每个分解后的字块在一定经纬度范围内具有相同的编码，也就是靠近的地点可能具有相同的编码。可以保护用户隐私、也比较容易做缓存。

## Bloom Filters

- 主要用于大数量的去重，时间空间效率均很高，但是存在误识别率
  - 有“假阳率” - 判定为存在的元素，可能不存在
  - 没有“假阴率” - 判定为不存在的元素，可能存在
  - that is，布隆过滤器判定为存在的元素可能不存在，但是判定为不存在的元素一定不存在

# 应用

---

- Hash table是一种实现[<u>associative array</u>](https://en.wikipedia.org/wiki/Associative_array)的方法。（另一种实现方法是search tree）
- 数据加密
  - [<u>密码传输</u>](https://www.zhihu.com/question/26762707/answer/890181997)：“客户端对用户输入的密码进行hash运算，然后在服务端的数据库中保存用户密码的hash值。由于服务器端也没有存储密码的明文，所以目前很多网站也就不再有找回密码的功能了”
- 数据校验
  - 数据完整性校验
  - 版权校验
  - 大文件分块校验
- 负载均衡


# 历史

---

TAOCP - P547 Hashing - History：  
- 思想首次提出 1953 H. P. Luhn, IBM。内部备忘录中，建议使用拉链，提出了在外部查找中以使用包含一个以上元素的桶为好。  
- 公开文献首次提到 1956 Arnold. I. Dumey, Computers And Automation。第一个提出除以一个素数和使用余数作为散列地址的思想。  
- 首个出版物 1967 H. Hellerman, Digital Computer System Principles（1967年以前hash已经成了键码转换的标准术语，但是一直没有人在出版物中公开使用这一词语）

# 参考资料
- 《The Art Of Computer Programming (Volume 3)》  
- 《算法导论》第11章-散列表  
- Wikipedia